{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from models import Baseline_Embeddings, Baseline_LSTM\n",
    "from utils import to_gpu, Corpus, batchify, SNLIDataset, collate_snli\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch baseline for Text')\n",
    "parser.add_argument('--data_path', type=str, default='./data/classifier',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--model_type', type=str, default=\"emb\",\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--epochs', type=int, default=20,\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--packed_rep', type=bool, default=True,\n",
    "                    help='pad all sentences to fixed maxlen')\n",
    "parser.add_argument('--train_mode', type=bool, default=True,\n",
    "                    help='set training mode')\n",
    "parser.add_argument('--maxlen', type=int, default=10,\n",
    "                    help='maximum sentence length')\n",
    "parser.add_argument('--lr', type=float, default=1e-05,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='seed')\n",
    "parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                    help='beta1 for adam. default=0.9')\n",
    "parser.add_argument('--cuda', action='store_true', default=True,\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--save_path', type=str, default='./models/baseline_try',\n",
    "                    help='used for saving the models')\n",
    "parser.add_argument('--vocab_size', type=int, default=11000,\n",
    "                    help='vocabulary size')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab 41574; pruned to 11004\n",
      "Number of sentences dropped from ./data/classifier/train.txt: 448221 out of 549367 total\n",
      "original vocab 41574; pruned to 11004\n",
      "Number of sentences dropped from ./data/classifier/test.txt: 8288 out of 9824 total\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (11000) must match the existing size (11004) at non-singleton dimension 0.  Target sizes: [11000, 100].  Tensor sizes: [11004, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b9da7166879f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbaseline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseline_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"emb\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbaseline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseline_Embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/natural-adversary/textSNLI/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, emb_size, vocab_size)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0membeddings_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_prem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_hypo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (11000) must match the existing size (11004) at non-singleton dimension 0.  Target sizes: [11000, 100].  Tensor sizes: [11004, 100]"
     ]
    }
   ],
   "source": [
    "corpus_train = SNLIDataset(train=True, vocab_size=args.vocab_size, path=args.data_path)\n",
    "corpus_test = SNLIDataset(train=False, vocab_size=args.vocab_size, path=args.data_path)\n",
    "trainloader= torch.utils.data.DataLoader(corpus_train, batch_size = args.batch_size, collate_fn=collate_snli, shuffle=True)\n",
    "train_iter = iter(trainloader)\n",
    "testloader= torch.utils.data.DataLoader(corpus_test, batch_size = args.batch_size, collate_fn=collate_snli, shuffle=False)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.model_type==\"lstm\":\n",
    "    baseline_model = Baseline_LSTM(100,300,maxlen=args.maxlen, gpu=args.cuda)\n",
    "elif args.model_type==\"emb\":\n",
    "    baseline_model = Baseline_Embeddings(100, vocab_size=args.vocab_size)\n",
    "    \n",
    "if args.cuda:\n",
    "    baseline_model = baseline_model.cuda()\n",
    "optimizer = optim.Adam(baseline_model.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           betas=(args.beta1, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "if args.train_mode:\n",
    "    for epoch in range(0, args.epochs):\n",
    "        niter = 0\n",
    "        loss_total = 0\n",
    "        while niter < len(trainloader):\n",
    "            niter+=1\n",
    "            premise, hypothesis, target = train_iter.next()\n",
    "            if args.cuda:\n",
    "                premise=premise.cuda()\n",
    "                hypothesis = hypothesis.cuda()\n",
    "                target = target.cuda()\n",
    "            prob_distrib = baseline_model.forward((premise, hypothesis))\n",
    "            loss=criterion(prob_distrib, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total += loss.data[0]\n",
    "        print(loss_total/float(niter))\n",
    "        train_iter = iter(trainloader)\n",
    "        curr_acc = evaluate_model()\n",
    "        if curr_acc > best_accuracy:\n",
    "            print(\"saving model...\")\n",
    "            with open(args.save_path+\"/\"+args.model_type+'.pt', 'wb') as f:\n",
    "                torch.save(baseline_model.state_dict(), f)\n",
    "            best_accuracy = curr_acc\n",
    "        \n",
    "    print(\"Best accuracy :{0}\".format(best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_train.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11008"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_train.dictionary.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11008"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_test.dictionary.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt1",
   "language": "python",
   "name": "pt1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
